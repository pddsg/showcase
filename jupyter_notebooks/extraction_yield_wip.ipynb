{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d0c9af-0b05-499e-b431-fd996edece6b",
   "metadata": {},
   "source": [
    "## Precursor to krill extraction yield calculations\n",
    "The script below integrates (as in \"calculates the area under the curve\") for a couple of tags relevant for yield estimation.<br>\n",
    "It is inspired by Lars' Grafana dashboard: https://akerbiomarine.grafana.net/d/cdw2jrph7gj5sf/1-houston-weekly-production-dashboard-ll?orgId=1&from=2025-06-02T05:00:00.000Z&to=2025-06-09T04:59:59.000Z&timezone=America%2FChicago<br>\n",
    "Only the trapezoidal rule is used for integration and no baseline correction is performed. <br>\n",
    "**This script does nto consider that there is a lag between extraction tank filling and decanter feeding.** Possibly, this cancels out for longer time intervals, or is at least less of a problem?<br>\n",
    "Following the integrations in the script below, one only needs to do the following to arrive at an estimate of krill oil extraction yield:\n",
    "\n",
    "* FI323113_EvaporatorThreeDischarge/(FI2120.PV-FI6320.PV)*100 = yield\n",
    "\n",
    "One needs to split the dataframe obtained via this Python script and perform the above calculation row for row on the individual integration results.<br><br>\n",
    "**One also needs to make sure that all data are from representative production period. If there are no extractions, the tag readings are not meaningful.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d4ddc-8b59-47fd-9bce-7d469430c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple\n",
    "\n",
    "# user defined variables\n",
    "START_TIME = datetime(2025, 5, 11, 11, 0, 0) # SQL‐query start AND first integration interval start, (yyyy, m, d, hr, min, sec)\n",
    "END_TIME = datetime(2025, 5, 15, 0, 0, 0) # SQL‐query end (and integration cutoff)\n",
    "INTERVAL_HOURS = 24\n",
    "# remember, time in data warehouse is UTC time\n",
    "\n",
    "tags_tables = [\n",
    "    (\"akbm-houston-prod.houston_data.sensor_data_scada\", \"FI2120.PV\"),  # decanter feed flow rate. must be the sum of ethanol and krill meal\n",
    "    (\"akbm-houston-prod.houston_data.sensor_data_scada\", \"FI323113_EvaporatorThreeDischarge\"), # finished krill oil product discharged from evap 3 in units of kg/hr\n",
    "    (\"akbm-houston-prod.houston_data.sensor_data_scada\", \"FI6320.PV\"), # ethanol used for extraction\n",
    "    (\"akbm-houston-prod.houston_data.sensor_data_sulzer2\", \"FIT_323113\"),  # Added lbs/hr signal\n",
    "]\n",
    "\n",
    "dsn = 'bq64_system'\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)\n",
    "\n",
    "def fetch_1min_series(conn, dataset_table, tagname, start_time, end_time):\n",
    "    t0 = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    t1 = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    sql = f\"\"\"\n",
    "    SELECT TIMESTAMP_TRUNC(time, MINUTE) AS ts_min, AVG(value) AS avg_value\n",
    "    FROM `{dataset_table}`\n",
    "    WHERE tagname = '{tagname}'\n",
    "    AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "    GROUP BY ts_min\n",
    "    ORDER BY ts_min\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df = df.rename(columns={\"ts_min\": \"timestamp\", \"avg_value\": \"value\"})\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.set_index(\"timestamp\").sort_index()\n",
    "    return df\n",
    "\n",
    "def build_intervals(start, end, interval_hours):\n",
    "    intervals = []\n",
    "    cursor = start\n",
    "    delta = timedelta(hours=interval_hours)\n",
    "    while cursor < end:\n",
    "        next_cursor = cursor + delta\n",
    "        intervals.append((cursor, min(next_cursor, end)))\n",
    "        cursor = next_cursor\n",
    "    return intervals\n",
    "\n",
    "def integrate_trapz(df, start, end):\n",
    "    df_int = df.loc[start:end - timedelta(minutes=1), \"value\"].dropna()\n",
    "    if df_int.empty:\n",
    "        return 0.0\n",
    "    t_secs = (df_int.index.view(np.int64) // 10 ** 9).astype(float)\n",
    "    t_hours = t_secs / 3600.0\n",
    "    y = df_int.values\n",
    "    integral = np.trapz(y, x=t_hours)\n",
    "    return integral\n",
    "\n",
    "all_records = []\n",
    "intervals = build_intervals(START_TIME, END_TIME, INTERVAL_HOURS)\n",
    "\n",
    "for table, tag in tags_tables:\n",
    "    df = fetch_1min_series(conn, table, tag, START_TIME, END_TIME)\n",
    "    for start, end in intervals:\n",
    "        integral = integrate_trapz(df, start, end)\n",
    "        all_records.append({\n",
    "            \"tagname\": tag,\n",
    "            \"interval_start\": start,\n",
    "            \"interval_end\": end,\n",
    "            \"integrated_value\": integral\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(all_records)\n",
    "summary_df[\"interval_start\"] = pd.to_datetime(summary_df[\"interval_start\"])\n",
    "summary_df[\"interval_end\"] = pd.to_datetime(summary_df[\"interval_end\"])\n",
    "summary_df = summary_df.sort_values([\"tagname\", \"interval_start\"]).reset_index(drop=True)\n",
    "\n",
    "summary_df.to_csv(\"integration_summary.csv\", index=False)\n",
    "print(f\"Integration summary written to 'integration_summary.csv' with {len(summary_df)} rows.\")\n",
    "\n",
    "summary_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
